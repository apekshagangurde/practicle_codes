{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616633fe-348f-40c8-a425-8b08b74bd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample random text (100 words)\n",
    "random_text = \"\"\"\n",
    "Difference between AI and ML: The goal of Artificial Intelligence is to create a machine that can mimic a human mind, and it needs learning capabilities as well. However, it is more than just about learning; it is also about knowledge representation, reasoning and abstract thinking. In contrast, Machine Learning is solely focused on writing software that can learn from past experiences. Besides, Machine Learning is more closely related to Data Mining and Statistics than it is to Artificial Intelligence.\n",
    "Machine Learning: Using data to answer questions. “Using data” is what is generally referred to as “training”, and also “answering questions” is referred to as “making predictions”, or “inference”.\n",
    "Machine Learning methods\n",
    "Even though there are a number of approaches are used in Machine Learning, the most popular ones are as follows:\n",
    "Supervised Learning\n",
    "Supervised Learning is where you teach and train the machine using data, which is well-labelled. This means that the data is already tagged with the correct answer and correct outcome. Therefore, the greater the data set the more the machine can learn about the subject. After the machine is trained, it is given new previously unseen data and learning algorithm. Afterward, using the past experiences gives you an outcome. Supervised learning is commonly used in applications where historical data predicts future events. For instance, it can anticipate when credit card transactions are likely to be fraudulent or which insurance customer is likely to file a claim.\n",
    "An example for Supervised Learning based on topics, picture by Google Developer Tutorial\n",
    "2. Unsupervised Learning\n",
    "Unsupervised Learning is where the machine is trained using a data set that does not have any labels or tags. The learning algorithms are never mentioned what the data represents. Unsupervised learning like listening to podcast in a foreign language, which you do not understand. In addition, you do not have any teacher and dictionary to what you are listening to. If you listen to just one podcast, it will not be much benefit to you. However, if you listen to hundreds of hours of those podcasts, your brain will start to form a model about how the language works you. Also, it will start to recognize patterns, and you will start to expect certain sounds. Initially, there are some techniques that are used in Unsupervised Learning such as self-organizing maps, nearest-neighbor mapping, and k-means clustering. The main goal is to explore the data and find some structure within.\n",
    "An example of clustering in k-means algorithm, picture by Google Developer Tutorial\n",
    "K-means clustering is one of the most popular unsupervised machine learning algorithms. that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean that is called cluster centers or cluster centroid.\n",
    "A cluster refers to a collection of data points aggregated together due to specific similarities.\n",
    "3. Reinforcement Learning\n",
    "Reinforcement Learning is similar to unsupervised learning in that the training data is unlabeled. However, when asked a question about the data the outcome will be graded. For instance, If the machine wins the game, then the result is trickled back down through the set of moves to reinforce the validity of those moves. This does no mean that the computer plays just one or two games. If it plays thousands even millions of games, cumulative effect of the reinforcement will create a winning strategy. The goal is for the agent (the learner or decision maker) to choose actions that maximize the expected reward over a given amount of time. The agent will reach the goal much faster by following a good policy. As a result, the objective in reinforcement learning is to learn the best policy. Reinforcement Learning is often used for robotics, gaming, and navigation.\n",
    "An example of Reinforcement Learning in dog training, picture is provided by this article\n",
    "4. Semi-supervised Learning\n",
    "Semi-supervised Learning is used for the same applications as Supervised Learning. However, it uses both labeled and unlabeled data for training. Basically, a small amount of labeled data with a large amount of unlabeled data are used in this approach because unlabeled data is less expensive and takes less effort to acquire. Additionally, this type of learning can be used with some methods such as classification, regression, and prediction. Semi-supervised Learning is useful when the cost related to labeling is extremely high to allow for a fully labeled training process. An examples of this approach is identifying a person’s face on a web cam.\n",
    "7 steps of Machine Learning\n",
    "Data Collection: The quantity and quality of your data will directly specify how good your predictive model can be.\n",
    "2. Data Preparation: Just having raw data is not very useful. In fact, the data require to be prepared, normalized, de-duplicated and errors should be removed. Visualization of the data can be used as a technique to find patterns and outliers to see if the required data has been collected or missed.\n",
    "3. Choose a Model: There are a number of different models for various tasks and goals. So, you should choose the right model for your purpose based on your business goal. Besides, you should make sure how much preparation the model needs, how accurate it is, and how scalable the model is.\n",
    "4. Train the Model: The objective of training is to answer a question or make a prediction correctly as often as possible. This means it uses your training data and incrementally enhance the predictions of the model. Each cycle of updating the weights and biases is considered as one training step.\n",
    "5. Evaluate the Model: Some metric or combination of metrics to measure objective performance of model are used in this step. It means you should test the model against previously unseen to see how it performs.\n",
    "6. Parameter Tuning: This step refers to hyper-parameter tuning, which might include number of training steps, learning rate, initialization values and distribution. In other words, you should set parameters to improve the process.\n",
    "7. Make Predictions: As we know, Machine Learning is using data to answer questions. Thus, Prediction or inference, is the final step where we get to answer some questions.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(random_text)\n",
    "\n",
    "# Initialize the NLTK Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Get the English stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize a list to store the preprocessed words\n",
    "preprocessed_words = []\n",
    "\n",
    "# Perform text preprocessing\n",
    "for word in words:\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    word = word.lower()\n",
    "    word = word.strip('.,?!-()[]{}\"\\'')\n",
    "\n",
    "    # Check if the word is not a stop word\n",
    "    if word not in stop_words:\n",
    "        # Stem the word\n",
    "        word = stemmer.stem(word)\n",
    "\n",
    "        # Add the preprocessed word to the list\n",
    "        preprocessed_words.append(word)\n",
    "\n",
    "# Join the preprocessed words back into a text\n",
    "preprocessed_text = \" \".join(preprocessed_words)\n",
    "\n",
    "# Print the original text and preprocessed text\n",
    "print(\"Original Text:\")\n",
    "print(random_text)\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4851e58-44a5-4ab8-aea8-5255b1b882e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download required NLTK data (you only need to do this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Simple text preprocessing function\n",
    "def preprocess(text):\n",
    "    # Tokenize text (split into words)\n",
    "    words = word_tokenize(text.lower())  # Convert text to lowercase\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "     # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return stemmed_words, lemmatized_words\n",
    "# Example text\n",
    "text = \"This is a simple example for text preprocessing, with stop word removal and stemming!\"\n",
    "stemmed, lemmatized = preprocess(text)\n",
    "# Display results\n",
    "print(\"Stemmed Text:    \", stemmed)\n",
    "print(\"Lemmatized Text: \", lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a6359-9d66-42b5-b283-a6cdca70e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted files \n",
    "import re\n",
    "import collections\n",
    "\n",
    "# Sample documents\n",
    "documents = {\n",
    "    1: \"This is the first document. It contains some words.\",\n",
    "    2: \"This is the second document. It also contains words.\",\n",
    "    3: \"The third document is different from the first two.\",\n",
    "    4: \"Inverted index is essential for document retrieval.\",\n",
    "}\n",
    "\n",
    "# Function to preprocess and tokenize text\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\w+', text) #finds all the words (tokens) in the text. It splits the text by spaces or punctuation and keeps only words (letters, numbers, etc.).\n",
    "    return tokens\n",
    "\n",
    "# Create an inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = collections.defaultdict(list)\n",
    "    for doc_id, document in documents.items(): #The function takes a documents dictionary, where each key is a document ID, and the value is the text of that document.\n",
    "        tokens = preprocess(document) #It processes each document by splitting the text into tokens (words).\n",
    "        for token in tokens:   #Then, for each token (word), the function adds the document ID to the inverted index. This way, each word will map to all the document IDs where it appears.\n",
    "            inverted_index[token].append(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Function to perform document retrieval\n",
    "def retrieve_documents(query, inverted_index):\n",
    "    query_tokens = preprocess(query)\n",
    "    result = set()\n",
    "\n",
    "    # Retrieve documents containing each query token\n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            if not result:\n",
    "                result = set(inverted_index[token])\n",
    "            else:\n",
    "                result = result.intersection(inverted_index[token])\n",
    "\n",
    "    return result\n",
    "\n",
    "# Build the inverted index\n",
    "inverted_index = build_inverted_index(documents)\n",
    "\n",
    "# Example queries\n",
    "query1 = input(\"Enter query: \")\n",
    "\n",
    "\n",
    "# Retrieve documents for the queries\n",
    "result1 = retrieve_documents(query1, inverted_index)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"Query:\", query1)\n",
    "print(\"Matching Documents:\", result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995bbc1-07a4-4b8b-bf05-461c6a5ed3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted files\n",
    "import re\n",
    "import collections\n",
    "\n",
    "# Sample documents\n",
    "documents = {\n",
    "    1: \"This is the first document. It contains some words.\",\n",
    "    2: \"This is the second document. It also contains words.\",\n",
    "    3: \"The third document is different from the first two.\",\n",
    "    4: \"Inverted index is essential for document retrieval.\",\n",
    "}\n",
    "\n",
    "# Function to preprocess and tokenize text\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "# Create an inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = collections.defaultdict(list)\n",
    "    for doc_id, document in documents.items():\n",
    "        tokens = preprocess(document)\n",
    "        for token in tokens:\n",
    "            inverted_index[token].append(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Function to perform document retrieval with detailed output\n",
    "def retrieve_documents(query, inverted_index):\n",
    "    query_tokens = preprocess(query)\n",
    "    result = set()\n",
    "    \n",
    "    print(\"\\nTokenize Query: query_tokens =\", query_tokens)  # Show query tokens\n",
    "\n",
    "    # Retrieve documents containing each query token\n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            matching_docs = inverted_index[token]\n",
    "            print(f\"For '{token}': The inverted index shows {matching_docs}\")  # Show matching documents for each token\n",
    "            if not result:\n",
    "                result = set(matching_docs)\n",
    "            else:\n",
    "                result = result.intersection(matching_docs)  # Intersection of results\n",
    "        else:\n",
    "            print(f\"For '{token}': No matching documents found.\")\n",
    "            return set()  # If any token is not found, return an empty result\n",
    "\n",
    "    return result\n",
    "\n",
    "# Build the inverted index\n",
    "inverted_index = build_inverted_index(documents)\n",
    "\n",
    "# Example query\n",
    "query1 = input(\"Enter query: \")\n",
    "\n",
    "# Retrieve documents for the query\n",
    "result1 = retrieve_documents(query1, inverted_index)\n",
    "\n",
    "# Display the final result\n",
    "print(\"\\nIntersection of matching documents gives:\", result1)\n",
    "print(\"Matching Documents:\", result1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa8771-5ca6-46fc-b9e9-e7a7df15465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted files\n",
    "import re\n",
    "import collections\n",
    "\n",
    "# Sample documents\n",
    "documents = {\n",
    "    1: \"This is the first document. It contains some words.\",\n",
    "    2: \"This is the second document. It also contains words.\",\n",
    "    3: \"The third document is different from the first two.\",\n",
    "    4: \"Inverted index is essential for document retrieval.\",\n",
    "}\n",
    "\n",
    "# Function to preprocess and tokenize text\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "# Create an inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = collections.defaultdict(list)\n",
    "    for doc_id, document in documents.items():\n",
    "        tokens = preprocess(document)\n",
    "        for token in tokens:\n",
    "            inverted_index[token].append(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Function to perform document retrieval with detailed output\n",
    "def retrieve_documents(query, inverted_index):\n",
    "    query_tokens = preprocess(query)\n",
    "    result = set()\n",
    "    \n",
    "    print(\"\\nTokenize Query: query_tokens =\", query_tokens)  # Show query tokens\n",
    "\n",
    "    # Retrieve documents containing each query token\n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            matching_docs = inverted_index[token]\n",
    "            print(f\"For '{token}': The inverted index shows {matching_docs}\")  # Show matching documents for each token\n",
    "            if not result:\n",
    "                result = set(matching_docs)\n",
    "            else:\n",
    "                result = result.intersection(matching_docs)  # Intersection of results\n",
    "        else:\n",
    "            print(f\"For '{token}': No matching documents found.\")\n",
    "            return set()  # If any token is not found, return an empty result\n",
    "\n",
    "    return result\n",
    "\n",
    "# Build the inverted index\n",
    "inverted_index = build_inverted_index(documents)\n",
    "\n",
    "# Print the inverted index\n",
    "print(\"Inverted Index Form:\")\n",
    "for token, doc_ids in inverted_index.items():\n",
    "    print(f\"{token}: {doc_ids}\")\n",
    "\n",
    "# Example query\n",
    "query1 = input(\"\\nEnter query: \")\n",
    "\n",
    "# Retrieve documents for the query\n",
    "result1 = retrieve_documents(query1, inverted_index)\n",
    "\n",
    "# Display the final result\n",
    "print(\"\\nIntersection of matching documents gives:\", result1)\n",
    "print(\"Matching Documents:\", result1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7d66f-d04b-4082-8e98-35e450dc06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted files\n",
    "from collections import defaultdict\n",
    "# Step 1: Build the inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)\n",
    "    \n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for term in set(doc.lower().split()):  # Tokenize and avoid duplicates\n",
    "            inverted_index[term].append(doc_id)\n",
    "    \n",
    "    return inverted_index\n",
    "# Step 2: Search for documents containing terms\n",
    "def search(query, inverted_index):\n",
    "    return [inverted_index.get(term, []) for term in query.lower().split()]\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"A brown fox is quick and jumps high\",\n",
    "]\n",
    "# Build and query the inverted index\n",
    "inverted_index = build_inverted_index(documents)\n",
    "query = \"brown dog\"\n",
    "result = search(query, inverted_index)\n",
    "# Display results\n",
    "for idx, term in enumerate(query.split()):\n",
    "    print(f\"Documents containing {term}:\", result[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ae1dc-1116-413c-ba9c-422e8f6f1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#email filtering \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "\n",
    "# Load the dataset (replace 'emails.csv' with your actual file path)\n",
    "data = pd.read_csv('emails.csv')\n",
    "data.head()\n",
    "\n",
    "# Split the data into features (text) and target (spam)\n",
    "X = data['text']\n",
    "y = data['spam']\n",
    "\n",
    "# Convert text to numerical features using TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()  #Tokenize the text (split it into words).\n",
    "#Remove stopwords (commonly used words like \"the\", \"is\", etc.).\n",
    "#Calculate the term frequency (TF) and inverse document frequency (IDF) for each word.\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "class_report = classification_report(y_test, y_pred, target_names=['Not Spam', 'Spam'])\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# Define a function to classify email subjects\n",
    "def classify_email(subject):\n",
    "    cleaned_subject = re.sub(r'^Subject:\\s*', '', subject)  # Remove \"Subject:\" prefix\n",
    "    vectorized_subject = vectorizer.transform([cleaned_subject])     # types of txt vectorization :- 1 bag of words , 2 tf-idf TF(t,d)= \n",
    "#Total number of terms in document d\n",
    "#Number of times term t appears in document d\n",
    "    prediction = svm_classifier.predict(vectorized_subject)\n",
    "    if prediction[0] == 1:\n",
    "        return \"Spam\"\n",
    "    else:\n",
    "        return \"Not Spam\"\n",
    "\n",
    "# Ask the user to enter an email subject\n",
    "user_input = input(\"Enter an email subject: \")\n",
    "classification_result = classify_email(user_input)\n",
    "print(\"Classification:\", classification_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83f45f-e494-442a-a30c-e49c0c1b00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#email filtering \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "df = pd.read_csv('emails.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Convert text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])  # 'text' column contains the email content\n",
    "y = df['spam']  # 'spam' column contains labels (1 for spam, 0 for ham)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Train the Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# Print one example from y_pred\n",
    "random_index = random.randint(0, len(y_pred) - 1)  # Get a random index\n",
    "print(f\"Example Prediction:\\nText: {X_test[random_index].toarray()}\\nPredicted Spam: {y_pred[random_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a57ed-e757-4664-9be2-146f36b9dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agglomerative hierarchical clustering \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\n",
    "# Load the credit card dataset\n",
    "data = pd.read_csv('BankChurners.csv')\n",
    "\n",
    "# Select relevant features\n",
    "X = data[['Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count', 'Months_Inactive_12_mon']]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "agg_labels = agg_clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Plot the dendrogram\n",
    "linked = linkage(X_scaled, 'ward')\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the clusters (using 2D projection for visualization)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=agg_labels, cmap='rainbow')\n",
    "plt.xlabel('Standardized Customer Age')\n",
    "plt.ylabel('Standardized Dependent Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf8dfb-e982-4d95-91db-8d5839bc70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agglomerative hierarchical clustering \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Apply Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "agg_labels = agg_clustering.fit_predict(X_scaled)\n",
    "# Plot the dendrogram\n",
    "linked = linkage(X_scaled, method='ward')\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Dendrogram for Agglomerative Clustering')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "# Plot the clusters (using 2D projection for visualization)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=agg_labels, cmap='rainbow')\n",
    "plt.title('Agglomerative Clustering on Standardized Iris Dataset')\n",
    "plt.xlabel('Standardized Feature 1 (Sepal Length)')\n",
    "plt.ylabel('Standardized Feature 2 (Sepal Width)')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf3a0e-f8f0-4c4e-878b-698f5611c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page ranking \n",
    "import numpy as np\n",
    "\n",
    "def page_rank(graph, damping_factor=0.85, max_iterations=100, tol=1e-6):\n",
    "    # Number of pages\n",
    "    num_pages = len(graph)\n",
    "\n",
    "    # Initialize the PageRank values\n",
    "    pagerank = np.ones(num_pages) / num_pages\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        new_pagerank = np.zeros(num_pages)\n",
    "        for i in range(num_pages):\n",
    "            for j in range(num_pages):\n",
    "                if graph[j][i]:\n",
    "                    new_pagerank[i] += pagerank[j] / sum(graph[j])\n",
    "\n",
    "        # Apply damping factor and update PageRank\n",
    "        new_pagerank = (1 - damping_factor) / num_pages + damping_factor * new_pagerank\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(new_pagerank - pagerank) < tol:\n",
    "            return new_pagerank\n",
    "\n",
    "        pagerank = new_pagerank\n",
    "\n",
    "    return pagerank\n",
    "\n",
    "# Example graph representing web page connections\n",
    "# Replace this with your own graph\n",
    "web_graph = [\n",
    "    [0, 1, 1, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [1, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "]\n",
    "\n",
    "pagerank_values = page_rank(web_graph)\n",
    "print(\"PageRank values:\", pagerank_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f7ff1-20f6-4095-8531-a3d3567992af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page ranking \n",
    "import numpy as np\n",
    "import networkx as nx #is a library for the creation, manipulation, and analysis of the structure and dynamics of complex networks (graphs).\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges to the graph (example graph)\n",
    "edges = [('A', 'B'), ('B', 'C'), ('C', 'A'), ('B', 'D')]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Compute PageRank\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Print the PageRank values\n",
    "for node, rank in pagerank.items():\n",
    "    print(f\"Node {node}: {rank:.4f}\")\n",
    "\n",
    "pip install networkx\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "# Add edges to the graph (example graph)\n",
    "edges = [('A', 'B'), ('B', 'C'), ('C', 'A'), ('B', 'D')]\n",
    "G.add_edges_from(edges)\n",
    "# Compute PageRank\n",
    "pagerank = nx.pagerank(G)\n",
    "# Print the PageRank values\n",
    "for node, rank in pagerank.items():\n",
    "    print(f\"Node {node}: {rank:.4f}\")\n",
    "\n",
    "# extra\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges to the graph (example graph)\n",
    "edges = [('A', 'B'), ('B', 'C'), ('C', 'A'), ('B', 'D')]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Draw the directed graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(G, with_labels=True, node_color='lightblue', edge_color='gray', node_size=3000, font_size=15, font_weight='bold', arrowsize=20)\n",
    "plt.title(\"Directed Graph Example\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15000420-a57b-4060-8e63-a30e59d7d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#web crawler \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the webpage you want to scrape\n",
    "url = \"https://webscraper.io/test-sites\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all anchor (a) tags with class 'site-heading' within a div with class 'test-site'\n",
    "    test_site_links = soup.find_all('div', class_='test-site')\n",
    "\n",
    "    # Extract and print the links\n",
    "    for site in test_site_links:\n",
    "        site_link = site.find('a')['href']\n",
    "        print(\"Test Site Link:\", site_link)\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "#extra\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the specific test site (E-commerce site)\n",
    "url = \"https://webscraper.io/test-sites/e-commerce/static/computers\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all product divs\n",
    "    product_divs = soup.find_all('div', class_='col-sm-4 col-lg-4 col-md-4')\n",
    "\n",
    "    \n",
    "    # Loop through each product div and extract product information\n",
    "    for product_div in product_divs:\n",
    "        # Extract product name\n",
    "        product_name = product_div.find('a', class_='card thumbnail').text.strip()\n",
    "\n",
    "        # Extract product price\n",
    "        product_price = product_div.find('h4', class_='pull-right').text.strip()\n",
    "\n",
    "        # Print or save the extracted data\n",
    "        print(\"Product Name:\", product_name)\n",
    "        print(\"Product Price:\", product_price)\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extra \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the specific test site (E-commerce site)\n",
    "url = \"https://webscraper.io/test-sites/e-commerce/allinone/computers\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all product divs\n",
    "    product_divs = soup.find_all('div', class_='card thumbnail')\n",
    "\n",
    "    # Loop through each product div and extract product information\n",
    "    for product_div in product_divs:\n",
    "        # Extract product name\n",
    "        product_name = product_div.find('a', class_='title').text.strip()\n",
    "        \n",
    "        # Extract product price\n",
    "        product_price = product_div.find('h4', class_='pull-right').text.strip()\n",
    "\n",
    "        # Print or save the extracted data\n",
    "        print(\"Product Name:\", product_name)\n",
    "        print(\"Product Price:\", product_price)\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99c4a14-5205-475a-b264-4af342fc912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#security measures \n",
    "# Import necessary libraries\n",
    "import hashlib  # For hashing to ensure data integrity\n",
    "from cryptography.fernet import Fernet  # For encryption\n",
    "import random  # For simulating sensor data\n",
    "\n",
    "# Generate an encryption key\n",
    "key = Fernet.generate_key()\n",
    "cipher = Fernet(key)\n",
    "\n",
    "\n",
    "def get_sensor_data():\n",
    "    return random.uniform(50, 100)  # Random temperature between 50°C and 100°C\n",
    "\n",
    "# Encrypt the sensor data\n",
    "def encrypt_data(data, cipher):\n",
    "    return cipher.encrypt(data.encode())\n",
    "\n",
    "# Decrypt the sensor data\n",
    "def decrypt_data(encrypted_data, cipher):\n",
    "    return cipher.decrypt(encrypted_data).decode()\n",
    "\n",
    "# Verify data integrity using hashing\n",
    "def verify_data_integrity(data):\n",
    "    return hashlib.sha256(data.encode()).hexdigest()\n",
    "\n",
    "\n",
    "sensor_data = f\"Temperature: {get_sensor_data():.2f}°C\"\n",
    "print(\"Original Data:\", sensor_data)\n",
    "\n",
    "# Encrypt and then decrypt the data\n",
    "encrypted_data = encrypt_data(sensor_data, cipher)\n",
    "print(\"Encrypted Data:\", encrypted_data)\n",
    "\n",
    "decrypted_data = decrypt_data(encrypted_data, cipher)\n",
    "print(\"Decrypted Data:\", decrypted_data)\n",
    "\n",
    "\n",
    "original_hash = verify_data_integrity(sensor_data)\n",
    "decrypted_hash = verify_data_integrity(decrypted_data)\n",
    "if original_hash == decrypted_hash:\n",
    "    print(\"Data Integrity Verified: Hashes match.\")\n",
    "else:\n",
    "    print(\"Data Integrity Issue: Hashes do not match.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5fb8b5-74da-4ad5-84d1-870cf094058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# industrial data \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = sns.load_dataset('diamonds')\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Step 1: Visualize the distribution of diamond prices (this can represent industrial product prices)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['price'], bins=30, kde=True)\n",
    "plt.title('Distribution of Diamond Prices')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Boxplot of price by cut type (similar to comparing industry sectors)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cut', y='price', data=data)\n",
    "plt.title('Diamond Prices by Cut Quality')\n",
    "plt.xlabel('Cut Quality')\n",
    "plt.ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='carat', y='price', hue='cut', data=data)\n",
    "plt.title('Carat vs Price of Diamonds')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 3: Visualize the relationship between carat and price with linear regression\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x='carat', y='price', data=data, scatter_kws={'alpha':0.3})\n",
    "plt.title('Carat vs Price with Linear Regression')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of correlations between numerical variables (handling non-numeric columns)\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Select only numeric columns for correlation calculation\n",
    "numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
    "correlation_matrix = numeric_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap of Numerical Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 5: Violin plot of price by color (distribution by color quality)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='color', y='price', data=data)\n",
    "plt.title('Diamond Prices by Color')\n",
    "plt.xlabel('Color')\n",
    "plt.ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55acd6-3087-413f-8311-238daddcd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIR sensor \n",
    "const int pirPin = 2;    // PIR sensor output pin\n",
    "const int ledPin = 13;   // LED pin\n",
    "const int buzzerPin = 8; // Buzzer pin\n",
    "\n",
    "bool motionTriggered = false; // Flag to track motion detection\n",
    "\n",
    "void setup() {\n",
    "  Serial.begin(9600);\n",
    "  pinMode(pirPin, INPUT);\n",
    "  pinMode(ledPin, OUTPUT);\n",
    "  pinMode(buzzerPin, OUTPUT);\n",
    "  digitalWrite(ledPin, LOW); // Ensure LED is off at start\n",
    "  noTone(buzzerPin);          // Ensure buzzer is off at start\n",
    "  Serial.println(\"PIR Sensor Setup Complete. Waiting for motion...\");\n",
    "}\n",
    "\n",
    "void loop() {\n",
    "  int motionDetected = digitalRead(pirPin);\n",
    "\n",
    "  if (motionDetected == HIGH && !motionTriggered) {\n",
    "    Serial.println(\"Motion Detected!\");\n",
    "    digitalWrite(ledPin, HIGH);   // Turn on LED\n",
    "    tone(buzzerPin, 1000);         // Sound buzzer at 1000 Hz\n",
    "    delay(1000);                   // Keep buzzer on for 1 second\n",
    "    noTone(buzzerPin);             // Turn off buzzer\n",
    "    motionTriggered = true;        // Set flag to indicate motion has been triggered\n",
    "  } else if (motionDetected == LOW) {\n",
    "    // Reset only if the buzzer and LED are off\n",
    "    if (motionTriggered) {\n",
    "      digitalWrite(ledPin, LOW);   // Turn off LED\n",
    "      motionTriggered = false;      // Reset the flag to allow new detection\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  delay(100); // Short delay for stability\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf52d1a-59a7-4acc-8226-887318eb9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alert msg \n",
    "\n",
    "Alert Message\n",
    "import RPi.GPIO as GPIO\n",
    "import time\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "\n",
    "SMTP_SERVER = 'smtp.gmail.com'\n",
    "SMTP_PORT = 587\n",
    "SMTP_USERNAME = 'kksupekar371121@kkwagh.edu.in'  \n",
    "SMTP_PASSWORD = 'Ksupekar@2003'  \n",
    "EMAIL_FROM = 'kksupekar371121@kkwagh.edu.in'  \n",
    "EMAIL_TO = 'darshankedare1815@gmail.com'  \n",
    "EMAIL_SUBJECT = 'Object Detected!'\n",
    "\n",
    "\n",
    "IR_SENSOR_PIN = 7\n",
    "LED_PIN = 11\n",
    "\n",
    "\n",
    "GPIO.setmode(GPIO.BOARD)\n",
    "GPIO.setup(IR_SENSOR_PIN, GPIO.IN)\n",
    "GPIO.setup(LED_PIN, GPIO.OUT)\n",
    "\n",
    "def send_email():\n",
    "    try:\n",
    "        # Create a secure SSL context\n",
    "        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n",
    "        server.starttls()\n",
    "        server.login(SMTP_USERNAME, SMTP_PASSWORD)\n",
    "\n",
    "        # Email content\n",
    "        message = MIMEMultipart()\n",
    "        message['From'] = EMAIL_FROM\n",
    "        message['To'] = EMAIL_TO\n",
    "        message['Subject'] = EMAIL_SUBJECT\n",
    "        body = \"An object has been detected by the IR sensor.\"\n",
    "        message.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "        # Send email\n",
    "        server.sendmail(EMAIL_FROM, EMAIL_TO, message.as_string())\n",
    "\n",
    "        # Clean up\n",
    "        server.quit()\n",
    "        print(\"Email sent successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if GPIO.input(IR_SENSOR_PIN) == GPIO.LOW:\n",
    "            print(\"Object detected!\")\n",
    "            GPIO.output(LED_PIN, GPIO.HIGH)  # Turn on LED\n",
    "            send_email()  # Send email notification\n",
    "            time.sleep(1)  # Delay to avoid multiple emails in quick succession\n",
    "        else:\n",
    "            print(\"No object detected.\")\n",
    "            GPIO.output(LED_PIN, GPIO.LOW)  # Turn off LED\n",
    "       \n",
    "        time.sleep(0.5)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nExiting program.\")\n",
    "finally:\n",
    "    GPIO.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e512f4-1583-4538-b9fd-551eeb0623df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thingspeak\n",
    "\n",
    "Program:\n",
    "import http.client , urllib.request,urllib.parse\n",
    "key = ' ' # copy the write API key\n",
    "\n",
    "t=int(22)\n",
    "\n",
    "while True:\n",
    "    t=t+1\n",
    "    print (\"temperature\",t)\n",
    "    params=urllib.parse.urlencode({'field1':t,'key':key})\n",
    "    headers={\"Content-type\":\"application/x-www-form-urlencoded\",\"Accept\":\"text/plain\"}\n",
    "    con=http.client.HTTPConnection(\"api.thingspeak.com\")\n",
    "    con.request(\"POST\",\"/update\",params,headers)\n",
    "    response=con.getresponse()\n",
    "    print (response.status,response.reason)\n",
    "    data=response.read()\n",
    "    con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1935d0-340d-4386-b336-535d672b53f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
